{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Shape Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nodes_train = np.load('./data/nodes_train.npy')\n",
    "shape_train = nodes_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component Analysis\n",
    "Re-arrange to put all the coodinates in individual vectors, so that we will have 200 of them. The take out the means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(nodes_train,[642*3,200],order='F')  # use Fortran-like index order for ease of reshaping\n",
    "mX = np.mean(X,axis=1,keepdims=True)\n",
    "X = X - mX\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute eigendecompose of the Gram matrix (instead of the covariance matrix when observation is limited for numerical stability and computational efficiency), such that dot(G, evs) = lambdas * evs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = np.matmul(np.transpose(X),X) / (shape_train[2]-1)\n",
    "lambdas, evs = np.linalg.eig(G)\n",
    "lambdas = np.clip(lambdas,1e-6,None)  # to numerically avoid negative variances\n",
    "print(lambdas.shape)\n",
    "print(evs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the numerical values of the eigendecomposition results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul(G,evs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul(evs,np.diag(lambdas)))  # lambdas*evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First normalise the eigenvectors to unit-length, the principal components PCs. Then, sort the PCs and eigenvectors evs in descending order per in lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs = np.matmul(np.matmul(X,evs),np.diag(1/np.sqrt((shape_train[2]-1)*lambdas)))\n",
    "idx_sorted = np.flip(np.argsort(lambdas),axis=0)\n",
    "lambdas = lambdas[idx_sorted]\n",
    "PCs = PCs[:,idx_sorted] \n",
    "print(PCs.shape)\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the SMM reconstruction B = PCs' * X  ->  X = PCs * B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPCs = 5\n",
    "B = np.matmul(np.transpose(PCs[:,0:nPCs]), X)  # projection\n",
    "X_recon = np.matmul(PCs[:,0:nPCs], B)  # reconstruction\n",
    "diff = X_recon-X\n",
    "print(PCs[:,0:nPCs].shape)\n",
    "print(np.sqrt(np.mean(np.square(diff))))  # in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the SSM \"model\" now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./data/my_smm.npz', mX, PCs, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY\n",
    "Now, given a fixed nPCs, change the weights of PCs between [-3 * sqrt(lambdas), 3 * sqrt(lambdas)]. One should expect to see the \"shape variance\" summarised by the SMM,\n",
    "e.g. <img src=\"./media/Deformation_smm.gif\" width=75% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "footnote: the PCA can also be performed by sigular value decomposition, which is numerically superior, while iterative methods, e.g. expectation-maximisation, also exists for additional computational gain or on-line performace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
